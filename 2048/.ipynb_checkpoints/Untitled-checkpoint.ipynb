{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game2048 import game,env_reset\n",
    "import logic\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = logic.start_game()\n",
    "m = np.array(m)\n",
    "game(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_o = {2: 2, 4: 1}\n",
    "\n",
    "d_n = {4: 2}\n",
    "\n",
    "rewards = 0\n",
    "for i,j in zip(d_n,d_o):\n",
    "    if i==j and (d_n[i]>d_o[j]):\n",
    "        rewards+=i\n",
    "for i in d_n:\n",
    "    if i not in d_o:\n",
    "        print(i)\n",
    "        rewards+=i\n",
    "for i in d_o:\n",
    "    if i not in d_n:\n",
    "        rewards+=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "\n",
    "enviroment = gym.make(\"Taxi-v3\").env\n",
    "#enviroment.render()\n",
    "\n",
    "print('Number of states: {}'.format(enviroment.observation_space.n))\n",
    "print('Number of actions: {}'.format(enviroment.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = (4*4)**2\n",
    "actions = 4\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "q_table = np.zeros([state, actions])\n",
    "num_of_episodes = 100\n",
    "actis = [logic.move_left,logic.move_right,logic.move_up,logic.move_down]\n",
    "state = 0\n",
    "for episode in range(num_of_episodes):\n",
    "    # Reset the enviroment\n",
    "    m = logic.start_game()\n",
    "    logic.add_new_2(m)\n",
    "    action = random.choice([0,1,2,3])\n",
    "    state = random.choice(range(0,255)\n",
    "    q_table[state][action] = 1\n",
    "    # Initialize variables\n",
    "    reward = 0\n",
    "    terminated = False\n",
    "    \n",
    "    while not terminated:\n",
    "        #print(m)\n",
    "        # Take learned path or explore new actions based on the epsilon\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = np.argmax(q_table[random.choice(range(0,255))])\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "        # Take action    \n",
    "        terminated = logic.get_current_state(m)\n",
    "        a = actis[action](m)\n",
    "        m, _ = a\n",
    "        print(a)\n",
    "        next_state = state+1\n",
    "        # Recalculate\n",
    "        q_value = q_table[state, action]\n",
    "        max_value = np.max(q_table[next_state])\n",
    "        new_q_value = (1 - alpha) * q_value + alpha * (reward + gamma * max_value)\n",
    "        \n",
    "        # Update Q-table\n",
    "        q_table[state, action] = new_q_value\n",
    "        state = next_state\n",
    "        \n",
    "    if (episode + 1) % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Episode: {}\".format(episode + 1))\n",
    "        #enviroment.render()\n",
    "\n",
    "print(\"**********************************\")\n",
    "print(\"Training is done!\\n\")\n",
    "print(\"**********************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = logic.start_game()\n",
    "logic.add_new_2(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = actis[1](m)\n",
    "m, _ = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(m)\n",
    "    action = random.choice([0,1,2,3])\n",
    "    a = actis[action](m)\n",
    "    m, _ = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 500\n",
      "Number of actions: 6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import gym\n",
    "from game2048 import game,env_reset\n",
    "import logic\n",
    "\n",
    "\n",
    "enviroment = gym.make(\"Taxi-v3\").env\n",
    "#enviroment.render()\n",
    "\n",
    "print('Number of states: {}'.format(enviroment.observation_space.n))\n",
    "print('Number of actions: {}'.format(enviroment.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "q_table = np.zeros([enviroment.observation_space.n, enviroment.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = enviroment.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_episodes = 100000\n",
    "\n",
    "for episode in range(0, num_of_episodes):\n",
    "    # Reset the enviroment\n",
    "    state = enviroment.reset()\n",
    "\n",
    "    # Initialize variables\n",
    "    reward = 0\n",
    "    terminated = False\n",
    "    \n",
    "    while not terminated:\n",
    "        # Take learned path or explore new actions based on the epsilon\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = enviroment.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state[1]])\n",
    "\n",
    "        # Take action    \n",
    "        next_state, reward, terminated, _,info = enviroment.step(action) \n",
    "        \n",
    "        # Recalculate\n",
    "        q_value = q_table[state, action]\n",
    "        max_value = np.max(q_table[next_state])\n",
    "        new_q_value = (1 - alpha) * q_value + alpha * (reward + gamma * max_value)\n",
    "        \n",
    "        # Update Q-table\n",
    "        q_table[state, action] = new_q_value\n",
    "        state = next_state\n",
    "        \n",
    "    if (episode + 1) % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Episode: {}\".format(episode + 1))\n",
    "        enviroment.render()\n",
    "\n",
    "print(\"**********************************\")\n",
    "print(\"Training is done!\\n\")\n",
    "print(\"**********************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100\n",
      "Episode: 200\n",
      "Episode: 300\n",
      "Episode: 400\n",
      "Episode: 500\n",
      "Episode: 600\n",
      "Episode: 700\n",
      "Episode: 800\n",
      "Episode: 900\n",
      "Episode: 1000\n",
      "Episode: 1100\n",
      "Episode: 1200\n",
      "Episode: 1300\n",
      "Episode: 1400\n",
      "Episode: 1500\n",
      "Episode: 1600\n",
      "Episode: 1700\n",
      "Episode: 1800\n",
      "Episode: 1900\n",
      "Episode: 2000\n",
      "Episode: 2100\n",
      "Episode: 2200\n",
      "Episode: 2300\n",
      "Episode: 2400\n",
      "Episode: 2500\n",
      "Episode: 2600\n",
      "Episode: 2700\n",
      "Episode: 2800\n",
      "Episode: 2900\n",
      "Episode: 3000\n",
      "Episode: 3100\n",
      "Episode: 3200\n",
      "Episode: 3300\n",
      "Episode: 3400\n",
      "Episode: 3500\n",
      "Episode: 3600\n",
      "Episode: 3700\n",
      "Episode: 3800\n",
      "Episode: 3900\n",
      "Episode: 4000\n",
      "Episode: 4100\n",
      "Episode: 4200\n",
      "Episode: 4300\n",
      "Episode: 4400\n",
      "Episode: 4500\n",
      "Episode: 4600\n",
      "Episode: 4700\n",
      "Episode: 4800\n",
      "Episode: 4900\n",
      "Episode: 5000\n",
      "Episode: 5100\n",
      "Episode: 5200\n",
      "Episode: 5300\n",
      "Episode: 5400\n",
      "Episode: 5500\n",
      "Episode: 5600\n",
      "Episode: 5700\n",
      "Episode: 5800\n",
      "Episode: 5900\n",
      "Episode: 6000\n",
      "Episode: 6100\n",
      "Episode: 6200\n",
      "Episode: 6300\n",
      "Episode: 6400\n",
      "Episode: 6500\n",
      "Episode: 6600\n",
      "Episode: 6700\n",
      "Episode: 6800\n",
      "Episode: 6900\n",
      "Episode: 7000\n",
      "Episode: 7100\n",
      "Episode: 7200\n",
      "Episode: 7300\n",
      "Episode: 7400\n",
      "Episode: 7500\n",
      "Episode: 7600\n",
      "Episode: 7700\n",
      "Episode: 7800\n",
      "Episode: 7900\n",
      "Episode: 8000\n",
      "Episode: 8100\n",
      "Episode: 8200\n",
      "Episode: 8300\n",
      "Episode: 8400\n",
      "Episode: 8500\n",
      "Episode: 8600\n",
      "Episode: 8700\n",
      "Episode: 8800\n",
      "Episode: 8900\n",
      "Episode: 9000\n",
      "Episode: 9100\n",
      "Episode: 9200\n",
      "Episode: 9300\n",
      "Episode: 9400\n",
      "Episode: 9500\n",
      "Episode: 9600\n",
      "Episode: 9700\n",
      "Episode: 9800\n",
      "Episode: 9900\n",
      "Episode: 10000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import gym\n",
    "from game2048 import game,env_reset\n",
    "import logic\n",
    "\n",
    "def state_2():\n",
    "    return random.choice(range(0,256)), random.choice([0,1,2,3])\n",
    "\n",
    "def state_(q_table):\n",
    "    action = random.choice([0,1,2,3])\n",
    "    state = random.choice(range(0,256))\n",
    "    q_table[state][action] = 1\n",
    "    return state,action\n",
    "\n",
    "def string_m(m):\n",
    "    s = \"\"\n",
    "    for i in range(len(m)):\n",
    "        for j in range(len(m[0])):\n",
    "            s+=str(m[i][j])\n",
    "    return s\n",
    "\n",
    "state = (4*4)**2\n",
    "actions = 4\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "q_table = np.zeros([state, actions])\n",
    "num_of_episodes = 10000\n",
    "actis = [logic.move_left,logic.move_right,logic.move_up,logic.move_down]\n",
    "\n",
    "\n",
    "\n",
    "for episode in range(num_of_episodes):\n",
    "    state, action = state_(q_table)\n",
    "    board = logic.start_game()\n",
    "    reward = 0\n",
    "    terminated = False\n",
    "    \n",
    "    while not terminated:\n",
    "\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice([0,1,2,3])\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "            \n",
    "        board = np.array(board)\n",
    "        o_s  = string_m(board)\n",
    "        terminated = logic.get_current_state(board)\n",
    "        if terminated:\n",
    "            break\n",
    "        board,_,reward = actis[action](board)\n",
    "        board = np.array(board)\n",
    "        m_s = string_m(board)\n",
    "        if o_s != m_s:\n",
    "            board = logic.add_new_2(board)\n",
    "        next_state,_ = state_(q_table)\n",
    "        q_value = q_table[state, action]\n",
    "        max_value = np.max(q_table[next_state])\n",
    "        new_q_value = (1 - alpha) * q_value + alpha * (reward + gamma * max_value)\n",
    "        q_table[state, action] = new_q_value\n",
    "        state = next_state\n",
    "    \n",
    "    if (episode + 1) % 100 == 0:\n",
    "        #clear_output(wait=True)\n",
    "        print(\"Episode: {}\".format(episode + 1))\n",
    "        #enviroment.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************\n",
      "Results\n",
      "**********************************\n",
      "Epochs per episode: 164.63\n",
      "Penalties per episode: 84.11\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 0\n",
    "total_penalties = 0\n",
    "num_of_episodes = 100\n",
    "\n",
    "for _ in range(num_of_episodes):\n",
    "    state, _ = state_2()\n",
    "    epochs = 0\n",
    "    penalties = 0\n",
    "    reward = 0\n",
    "    board = logic.start_game()\n",
    "    terminated = False\n",
    "    \n",
    "    while not terminated:\n",
    "        board = np.array(board)\n",
    "        \n",
    "        o_s  = string_m(board)\n",
    "        action = np.argmax(q_table[state])\n",
    "        board,_,reward = actis[action](board)\n",
    "        terminated = logic.get_current_state(board)\n",
    "        m_s = string_m(board)\n",
    "        state,_ = state_2()\n",
    "        if terminated:\n",
    "            break\n",
    "        if o_s != m_s:\n",
    "            board = logic.add_new_2(board)\n",
    "        if reward == 0:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "    #print(board)\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(\"**********************************\")\n",
    "print(\"Results\")\n",
    "print(\"**********************************\")\n",
    "print(\"Epochs per episode: {}\".format(total_epochs / num_of_episodes))\n",
    "print(\"Penalties per episode: {}\".format(total_penalties / num_of_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 1.        , 5.91483504],\n",
       "       [1.        , 1.12590785, 1.        , 1.        ],\n",
       "       [0.96271742, 0.96      , 1.        , 1.        ],\n",
       "       ...,\n",
       "       [1.        , 3.46348001, 1.        , 1.        ],\n",
       "       [1.57267609, 1.        , 1.        , 1.        ],\n",
       "       [1.        , 1.        , 4.889851  , 1.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0], [0, 0, 0, 2], [0, 0, 0, 0], [0, 0, 0, 2]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board = logic.start_game()\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3, 3]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(([1,2,3],[1,3,3]),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(4,4)\n",
    "#16\n",
    "#256\n",
    "#4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import logic\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.FT = nn.Flatten()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ln = nn.Linear(16,256)\n",
    "        self.ln2 = nn.Linear(256,4)\n",
    "    def forward(self,x):\n",
    "        x = self.FT(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.relu(x)\n",
    "        return self.ln2(x)\n",
    "\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "\n",
    "def string_m(m):\n",
    "    s = \"\"\n",
    "    for i in range(len(m)):\n",
    "        for j in range(len(m[0])):\n",
    "            s+=str(m[i][j])\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = optim.RMSprop(policy_net.parameters())\n",
    "\n",
    "state = (4*4)**2\n",
    "actions = 4\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "q_table = np.zeros([state, actions])\n",
    "num_of_episodes = 50\n",
    "actis = [logic.move_left,logic.move_right,logic.move_up,logic.move_down]\n",
    "\n",
    "def select_action(state):\n",
    "    sample = random.random()\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(4)]], device = device ,dtype = torch.long)\n",
    "\n",
    "for episode in range(num_of_episodes):\n",
    "    \n",
    "    state = random.choice(range(0,256))\n",
    "    board = logic.start_game()\n",
    "    \n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        o_s  = string_m(board)\n",
    "        terminated = logic.get_current_state(board)\n",
    "        if terminated:\n",
    "            break\n",
    "        board,_,reward = actis[action](board)\n",
    "        m_s = string_m(board)\n",
    "        if o_s != m_s:\n",
    "            board = logic.add_new_2(board)\n",
    "        next_state = random.choice(range(0,256))\n",
    "        memory.push(state,action,next_state,reward)\n",
    "        state = next_state\n",
    "        optimize_model()\n",
    "        if t%10 == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    if (episode + 1) % 1 == 0:\n",
    "        print(\"Episode: {}\".format(episode + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
